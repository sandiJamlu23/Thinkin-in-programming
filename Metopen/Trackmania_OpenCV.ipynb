{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps were taken in the project:\n",
    "1. Denoising the video frames using blurring kernel;\n",
    "2. rayscaling and detecting edges on the frames with Canny edge detection;\n",
    "3. Drawing region of interest to embed lanes on the video frame;\n",
    "4. Perspective warping;\n",
    "5. Segmentation of lanes using vertical histogram projection;\n",
    "6. Detecting lines on the video frame using Hough Lines Polar and line optimization;\n",
    "7. Displaying lines on the frame;\n",
    "8. Turn prediction;\n",
    "9. Whole process orchestrator;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/tutorial-build-a-lane-detector-679fd8953132"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://automaticaddison.com/the-ultimate-guide-to-real-time-lane-detection-using-opencv/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://shirolepranav.github.io/blog/computer%20vision/opencv/2020/08/30/lane-detection.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import os\n",
    "import math\n",
    "import PIL as ImageGrab\n",
    "import pyautogui as py\n",
    "import pydirectinput\n",
    "import keyboard\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Screenshot 1 saved at my_game_screenshots\\screenshot_1.png\n",
      "Screenshot 2 saved at my_game_screenshots\\screenshot_2.png\n",
      "Screenshot 3 saved at my_game_screenshots\\screenshot_3.png\n",
      "Screenshot 4 saved at my_game_screenshots\\screenshot_4.png\n",
      "Screenshot 5 saved at my_game_screenshots\\screenshot_5.png\n",
      "Screenshot 6 saved at my_game_screenshots\\screenshot_6.png\n",
      "Screenshot 7 saved at my_game_screenshots\\screenshot_7.png\n",
      "Screenshot 8 saved at my_game_screenshots\\screenshot_8.png\n",
      "Screenshot 9 saved at my_game_screenshots\\screenshot_9.png\n",
      "Screenshot 10 saved at my_game_screenshots\\screenshot_10.png\n",
      "Screenshot 11 saved at my_game_screenshots\\screenshot_11.png\n",
      "Screenshot 12 saved at my_game_screenshots\\screenshot_12.png\n",
      "Screenshot 13 saved at my_game_screenshots\\screenshot_13.png\n",
      "Screenshot 14 saved at my_game_screenshots\\screenshot_14.png\n",
      "Screenshot 15 saved at my_game_screenshots\\screenshot_15.png\n",
      "Screenshot 16 saved at my_game_screenshots\\screenshot_16.png\n",
      "Screenshot 17 saved at my_game_screenshots\\screenshot_17.png\n",
      "Screenshot 18 saved at my_game_screenshots\\screenshot_18.png\n",
      "Screenshot 19 saved at my_game_screenshots\\screenshot_19.png\n",
      "Screenshot 20 saved at my_game_screenshots\\screenshot_20.png\n",
      "Screenshot 21 saved at my_game_screenshots\\screenshot_21.png\n",
      "Screenshot 22 saved at my_game_screenshots\\screenshot_22.png\n",
      "Screenshot 23 saved at my_game_screenshots\\screenshot_23.png\n",
      "Screenshot 24 saved at my_game_screenshots\\screenshot_24.png\n",
      "Screenshot 25 saved at my_game_screenshots\\screenshot_25.png\n",
      "Screenshot 26 saved at my_game_screenshots\\screenshot_26.png\n",
      "Screenshot 27 saved at my_game_screenshots\\screenshot_27.png\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_screenshots\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m screenshots captured and saved in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Example: Take 10 screenshots with a 1-second delay between each, store them in \"my_game_screenshots\" directory\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m \u001b[43mcapture_screenshots\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmy_game_screenshots\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 17\u001b[0m, in \u001b[0;36mcapture_screenshots\u001b[1;34m(num_screenshots, delay, save_dir)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mScreenshot \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m saved at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;66;03m# Add delay between screenshots\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(delay)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_screenshots\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m screenshots captured and saved in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def capture_screenshots(num_screenshots, delay=1, save_dir=\"screenshots\"):\n",
    "    # Create the directory if it doesn't exist\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    \n",
    "    for i in range(num_screenshots):\n",
    "        # Capture screenshot\n",
    "        screenshot = py.screenshot(region=(5,30, 847, 542))\n",
    "        \n",
    "        # Save the screenshot to the specified directory with a unique filename\n",
    "        save_path = os.path.join(save_dir, f'screenshot_{i+1}.png')\n",
    "        screenshot.save(save_path)\n",
    "        \n",
    "        print(f'Screenshot {i+1} saved at {save_path}')\n",
    "        \n",
    "        # Add delay between screenshots\n",
    "        time.sleep(delay)\n",
    "    \n",
    "    print(f\"{num_screenshots} screenshots captured and saved in {save_dir}!\")\n",
    "\n",
    "# Example: Take 10 screenshots with a 1-second delay between each, store them in \"my_game_screenshots\" directory\n",
    "capture_screenshots(50, delay=1, save_dir=\"my_game_screenshots\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[243, 243, 243],\n",
       "        [243, 243, 243],\n",
       "        [243, 243, 243],\n",
       "        ...,\n",
       "        [ 24,  24,  24],\n",
       "        [ 24,  24,  24],\n",
       "        [ 24,  24,  24]],\n",
       "\n",
       "       [[243, 243, 243],\n",
       "        [243, 243, 243],\n",
       "        [243, 243, 243],\n",
       "        ...,\n",
       "        [ 24,  24,  24],\n",
       "        [ 24,  24,  24],\n",
       "        [ 24,  24,  24]],\n",
       "\n",
       "       [[243, 243, 243],\n",
       "        [243, 243, 243],\n",
       "        [243, 243, 243],\n",
       "        ...,\n",
       "        [ 24,  24,  24],\n",
       "        [ 24,  24,  24],\n",
       "        [ 24,  24,  24]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 80, 148,  48],\n",
       "        [ 79, 144,  49],\n",
       "        [ 79, 146,  50],\n",
       "        ...,\n",
       "        [ 24,  24,  24],\n",
       "        [ 24,  24,  24],\n",
       "        [ 24,  24,  24]],\n",
       "\n",
       "       [[ 80, 140,  52],\n",
       "        [ 82, 142,  53],\n",
       "        [ 82, 144,  51],\n",
       "        ...,\n",
       "        [ 24,  24,  24],\n",
       "        [ 24,  24,  24],\n",
       "        [ 24,  24,  24]],\n",
       "\n",
       "       [[ 83, 140,  54],\n",
       "        [ 84, 140,  52],\n",
       "        [ 82, 140,  50],\n",
       "        ...,\n",
       "        [ 24,  24,  24],\n",
       "        [ 24,  24,  24],\n",
       "        [ 24,  24,  24]]], dtype=uint8)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def takeSS():\n",
    "    im = py.screenshot('image.png',region=(5,30, 847, 542))\n",
    "    # Change the format that OpenCV can understand\n",
    "    img = np.array(im)\n",
    "    img_cvt = cv.cvtColor(img, cv.COLOR_RGB2BGR)\n",
    "\n",
    "    # cv.imshow(\"das\", img_cvt)\n",
    "    # cv.waitKey(0)\n",
    "    # cv.destroyAllWindows()\n",
    "    return img_cvt\n",
    "\n",
    "takeSS()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brightness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scalling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple threshold for segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv.imread('D:/Thinkin in programming/Metopen/image.png', cv.IMREAD_GRAYSCALE)\n",
    "_, binary_image = cv.threshold(img, 127, 255, cv.THRESH_BINARY)\n",
    "\n",
    "cv.imshow('img',img)\n",
    "cv.waitKey(0)\n",
    "cv.imshow('global threshold',binary_image)\n",
    "\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Edge detection using Canny Edge Detection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def canny_edge_detection(img):\n",
    "\n",
    "    gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY )\n",
    "    kernel = 5\n",
    "    blur = cv.GaussianBlur(gray, (kernel, kernel),0)\n",
    "    canny = cv.Canny(blur, 50, 150)\n",
    "    \n",
    "    return canny\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specify the region of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roi(img):\n",
    "    height = img.shape[0]\n",
    "    width = img.shape[1]\n",
    "    \n",
    "    # Road mask\n",
    "    mask = np.zeros_like(img)\n",
    "    # car_mask = np.zeros_like(img)\n",
    "    \n",
    "    # Define the coordinates for the trapezoid region\n",
    "    # Adjust these points based on the shape of the lane and the car's position\n",
    "    roi_points = np.array([\n",
    "    [((250), (250)),    # Top-left corner\n",
    "     ((600), (250)),    # Top-right corner     \n",
    "     ((800), (300)),    # Bottom-right corner\n",
    "     ((50),  (300))]    # Bottom-left corner    \n",
    "    ], np.int32)\n",
    "\n",
    "    # Fill the triangular region\n",
    "    cv.fillPoly(mask, roi_points, 255)\n",
    "    #cv.fillPoly(mask, [car_triangle],0)\n",
    "    \n",
    "    # Use bitwise_and to apply the mask\n",
    "    masked_image = cv.bitwise_and(img, mask)\n",
    "    \n",
    "    # cv.imshow(\"ROI Applied\", masked_image)\n",
    "    # cv.waitKey(0)\n",
    "    # cv.destroyAllWindows()\n",
    "    \n",
    "    return masked_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'takeSS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Example usage:\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m img_cvt \u001b[38;5;241m=\u001b[39m \u001b[43mtakeSS\u001b[49m()  \u001b[38;5;66;03m# Capture screenshot and convert\u001b[39;00m\n\u001b[0;32m     26\u001b[0m img_with_triangle \u001b[38;5;241m=\u001b[39m draw_road_roi(img_cvt)\n\u001b[0;32m     28\u001b[0m cv\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRoad Roi\u001b[39m\u001b[38;5;124m\"\u001b[39m, img_with_triangle)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'takeSS' is not defined"
     ]
    }
   ],
   "source": [
    "def draw_road_roi(img):\n",
    "    height, width = img.shape[0], img.shape[1]\n",
    "\n",
    "    # Define a triangle that captures the road area in front of the car\n",
    "    # triangle = np.array([\n",
    "    #     [(int(width * 0.1), height),           # Bottom-left: 10% from the left of the frame\n",
    "    #      (int(width * 0.5), int(height * 0.6)), # Top-middle: center of the frame, 60% up\n",
    "    #      (int(width * 1.2), height)]           # Bottom-right: 90% from the left of the frame\n",
    "    # ], np.int32)\n",
    "    # HARDCODE\n",
    "    roi_points = np.array([\n",
    "    [((250), (250)),    # Top-left corner\n",
    "     ((600), (250)),    # Top-right corner     \n",
    "     ((800), (300)),    # Bottom-right corner\n",
    "     ((50),  (300))]    # Bottom-left corner    \n",
    "    ], np.int32)\n",
    "\n",
    "    # Draw the triangle on the image for visualization\n",
    "    cv.polylines(img, [roi_points], isClosed=True, color=(0, 255, 0), thickness=2)\n",
    "    return img\n",
    "\n",
    "# Example usage:\n",
    "img_cvt = takeSS()  # Capture screenshot and convert\n",
    "img_with_triangle = draw_road_roi(img_cvt)\n",
    "\n",
    "cv.imshow(\"Road Roi\", img_with_triangle)\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coordinate Explanation:\n",
    "* (int(width * 0.1), height) defines the bottom-left point of the triangle (10% of the frame width from the left, and at the bottom).\n",
    "* (int(width * 0.5), int(height * 0.5)) is the top-middle point of the triangle (center of the width, and 50% down from the top).\n",
    "* (int(width * 0.9), height) is the bottom-right point of the triangle (90% of the frame width from the left, and at the bottom).\n",
    "How to Adjust the Triangle:\n",
    "Visually Inspect the Lane:\n",
    "\n",
    "Run the program and see where the triangle is placed. \n",
    "1. Adjust the width * 0.1 and width * 0.9 values to move the triangle left or right.\n",
    "2. Adjust height * 0.5 to raise or lower the top point of the triangle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the HLD (Hough Line Detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def houghlines(img):\n",
    "    hough_lines = cv.HoughLinesP(\n",
    "        img,\n",
    "        rho=6,  # Resolution of the accumulator in pixels\n",
    "        theta=np.pi / 100,  # Angle resolution in radians\n",
    "        threshold=150,  # Minimum number of intersecting points\n",
    "        minLineLength=30,  # Minimum length of a line\n",
    "        maxLineGap=10  # Maximum allowed gap between line segments\n",
    "    )\n",
    "    return hough_lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as  cv\n",
    "\n",
    "# img = cv.imread('C:/Users/acer/Downloads/sudoku.png')\n",
    "img = cv.imread('D:/Thinkin in programming/Metopen/Roi_img.jpg')\n",
    "grays = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "blur = cv.GaussianBlur(grays, (5, 5),0)\n",
    "edges = cv.Canny(blur, 30,100,apertureSize=3)\n",
    "\n",
    "lines = cv.HoughLinesP(edges, 6, np.pi/80, threshold=100, minLineLength=40, maxLineGap=25)\n",
    "\n",
    "for line in lines:\n",
    "    x1,y1,x2,y2 = line[0]\n",
    "    cv.line(img,(x1,y1),(x2,y2),(0,255,0),2)\n",
    "# lines = cv.HoughLines(edges,1,np.pi/180,100)\n",
    "# for rho,theta in lines[0]:\n",
    "#     a = np.cos(theta)\n",
    "#     b = np.sin(theta)\n",
    "#     x0 = a*rho\n",
    "#     y0 = b*rho\n",
    "#     x1 = int(x0 + 1000 * (-b))\n",
    "#     y1 = int(y0 + 1000 * (a))\n",
    "#     x2 = int(x0 - 1000 * (-b))\n",
    "#     y2 = int(y0 - 1000 * (a))\n",
    "\n",
    "#     cv.line(img, (x1,y1), (x2,y2),(0,0,255),2)\n",
    "\n",
    "cv.imshow(\"line\",img)\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Histogram\n",
    "https://github.com/Infinem/Invenimus-Project/blob/master/lane_detection_with_memory/lane_detection_algorithm.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def histogram (img):\n",
    "    histogram = np.sum(img, axis = 0)\n",
    "\n",
    "    # Find mid point on histogram\n",
    "    midpoint = np.int32(histogram.shape[0]/2)\n",
    "\n",
    "    # Compute the left max\n",
    "    left_X = np.argmax(histogram[:midpoint])\n",
    "    right_X = np.argmax(histogram[midpoint:]) + midpoint\n",
    "\n",
    "    return left_X, right_X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with the following arguments:\n",
    "* dst: Output of the edge detector. It should be a grayscale image (although in fact it is a binary one)\n",
    "* lines: A vector that will store the parameters (r,θ) of the detected lines\n",
    "* rho : The resolution of the parameter r in pixels. We use 1 pixel.\n",
    "* theta: The resolution of the parameter θ in radians. We use 1 degree (CV_PI/180)\n",
    "* threshold: The minimum number of intersections to \"*detect*\" a line\n",
    "* srn and stn: Default parameters to zero. Check OpenCV reference for more info."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perspective Correction (https://medium.com/nerd-for-tech/lane-detection-with-opencv-part-1-ad9ea5758c07)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we were flying over the road, and watching it from a bird’s eye view, the lanes would be parallel, but in the picture, they are not, because of the perspective.\n",
    "The perspective depends on the focal length of the lens (lenses with a shorter focal length show a stronger perspective) and the position of the camera. Once the camera is mounted on a car, the perspective is fixed, so we can take it into consideration and correct the image.\n",
    "OpenCV has a method to compute the perspective transformation:\n",
    "**getPerspectiveTransform().**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is still wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perspective_wrapping(img):\n",
    "    # Get the height and width of the image.\n",
    "    height, width = img.shape[:2]  # Use [:2] to account for color images\n",
    "    # height, width = img.shape\n",
    "    # Set an offset value to tweak the perspective view window.\n",
    "    offset = 50\n",
    "\n",
    "    # Perspective points (source points) from the original image.\n",
    "    # These are the four points you want to transform from the source image.\n",
    "    source_points = np.array([\n",
    "        [250, 250],   # Top-left corner\n",
    "        [600, 250],   # Top-right corner     \n",
    "        [800, 300],   # Bottom-right corner\n",
    "        [50, 300]     # Bottom-left corner    \n",
    "    ], np.float32)  # Use float32 for getPerspectiveTransform\n",
    "\n",
    "    # The destination points for the perspective transform, where you want\n",
    "    # the source points to be mapped to in the resulting image.\n",
    "    # The offset is used to shrink the window a bit.\n",
    "    destination_points = np.array([\n",
    "        [offset, 0],                    # Top-left corner\n",
    "        [width - 2 * offset, 0],        # Top-right corner\n",
    "        [offset, height],               # Bottom-left corner\n",
    "        [width - 2 * offset, height]    # Bottom-right corner\n",
    "    ], np.float32)\n",
    "    # Changed the data type to np.float32 for both source_points and destination_points,\n",
    "    # which is required by cv.getPerspectiveTransform.\n",
    "\n",
    "    # Create the perspective transformation matrix using the source points \n",
    "    # and the destination points.\n",
    "    matrix = cv.getPerspectiveTransform(source_points, destination_points)\n",
    "\n",
    "    # Apply the perspective warp transformation to the image, generating a \n",
    "    # \"skyview\" of the image.\n",
    "    skyview = cv.warpPerspective(img, matrix, (width, height))\n",
    "\n",
    "    return skyview\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Line Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_points(img, lineSI):\n",
    "    slope, intercept = lineSI\n",
    "    # height = roi_bottom  # Use the bottom coordinate of the ROI\n",
    "    y1 = int(img.shape[0])  # Bottom of the ROI\n",
    "    y2 = int(y1 * 0.5)  # Extend the line slightly beyond the bottom of the ROI\n",
    "\n",
    "    # Calculate x coordinates based on slope and intercept\n",
    "    x1 = int((y1 - intercept) / slope)\n",
    "    x2 = int((y2 - intercept) / slope)\n",
    "\n",
    "    return [[x1, y1, x2, y2]]  # Return the points in the format required by cv.line\n",
    "\n",
    "\n",
    "# Before running this code below, you need to make sure that the line of the lane and the car\n",
    "# is already correct. Specify the correct region of interest first\n",
    "def average_slope_intercepts(img, lines):\n",
    "    \n",
    "    # lane_lines = [] # Both lines\n",
    "\n",
    "    left_fit = []\n",
    "    right_fit = []\n",
    "    \n",
    "    if lines is None:\n",
    "        return None\n",
    "    \n",
    "    for line in lines:\n",
    "        for x1, y1, x2, y2 in line:\n",
    "            # Check for a valid line length (avoid horizontal/vertical short lines)\n",
    "            fit = np.polyfit((x1, x2), (y1, y2), 1)\n",
    "            slope = fit[0]\n",
    "            intercept = fit[1]\n",
    "            # Filter based on slope: negative for left, positive for right\n",
    "            if slope < 0:  # Left lane\n",
    "                left_fit.append((slope, intercept))\n",
    "            elif slope > 0:  # Right lane\n",
    "                right_fit.append((slope, intercept))\n",
    "            # if abs(x2 - x1) > 0 and abs(y2 - y1) > 0:  \n",
    "                \n",
    "    left_fit_average  = np.average(left_fit, axis=0)\n",
    "    right_fit_average = np.average(right_fit, axis=0)\n",
    "    left_line  = make_points(img, left_fit_average)\n",
    "    right_line = make_points(img, right_fit_average)\n",
    "    average_lines = [left_line, right_line]\n",
    "    \n",
    "    # # Average fits\n",
    "    # if left_fit:\n",
    "    #     left_fit_average = np.average(left_fit, axis=0)\n",
    "    # else:\n",
    "    #     left_fit_average = None\n",
    "\n",
    "    # if right_fit:\n",
    "    #     right_fit_average = np.average(right_fit, axis=0)\n",
    "    # else:\n",
    "    #     right_fit_average = None\n",
    "\n",
    "    # # Generate lines only if averages are available\n",
    "    # average_lines = []\n",
    "    # if left_fit_average is not None:\n",
    "    #     left_line = make_points(img, left_fit_average, roi_bottom)\n",
    "    #     average_lines.append(left_line)\n",
    "    # if right_fit_average is not None:\n",
    "    #     right_line = make_points(img, right_fit_average, roi_bottom)\n",
    "    #     average_lines.append(right_line)\n",
    "\n",
    "    return average_lines\n",
    "\n",
    "\n",
    "def display_lines_average(img, lines):\n",
    "    # line_image = [0]\n",
    "    line_image = np.zeros_like(img)\n",
    "    if lines is not None:\n",
    "        for line in lines:\n",
    "            for x1, y1, x2, y2 in line:\n",
    "                cv.line(line_image, (x1,y1), (x2,y2), (0,0,255),10)\n",
    "    # return img\n",
    "    return line_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'perspective_wrapping' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m canny_edge \u001b[38;5;241m=\u001b[39m canny_edge_detection(image)\n\u001b[0;32m      6\u001b[0m masked_image \u001b[38;5;241m=\u001b[39m roi(canny_edge)\n\u001b[1;32m----> 8\u001b[0m wrap \u001b[38;5;241m=\u001b[39m \u001b[43mperspective_wrapping\u001b[49m(canny_edge)\n\u001b[0;32m     10\u001b[0m left_x, right_x \u001b[38;5;241m=\u001b[39m histogram(wrap)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# cv.imshow(\"hist\",left_x,right_x)\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'perspective_wrapping' is not defined"
     ]
    }
   ],
   "source": [
    "# img = takeSS()\n",
    "img = cv.imread(\"D:/Thinkin in programming/Metopen/my_game_screenshots/screenshot_47.png\")\n",
    "image = np.copy(img)\n",
    "\n",
    "canny_edge = canny_edge_detection(image)\n",
    "masked_image = roi(canny_edge)\n",
    "\n",
    "wrap = perspective_wrapping(canny_edge)\n",
    "\n",
    "left_x, right_x = histogram(wrap)\n",
    "# cv.imshow(\"hist\",left_x,right_x)\n",
    "\n",
    "cv.imshow(\"canny\",canny_edge)\n",
    "cv.imshow(\"wrap\",wrap)\n",
    "cv.imshow(\"ROI\",masked_image)\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img = takeSS()\n",
    "img = cv.imread(\"D:/Thinkin in programming/Metopen/my_game_screenshots/screenshot_47.png\")\n",
    "image = np.copy(img)\n",
    "canny_image = canny_edge_detection(image)\n",
    "masked_image =  roi(canny_image)\n",
    "\n",
    "# wrap = perspective_wrapping(masked_image)\n",
    "h_lines = houghlines(masked_image)\n",
    "# h_lines = houghlines(wrap)\n",
    "# cv.imshow(\"\",wrap)\n",
    "# cv.waitKey\n",
    "\n",
    "# Call the line function\n",
    "avg_lines = average_slope_intercepts(img, h_lines)\n",
    "lines_image = display_lines_average(image, avg_lines)\n",
    "\n",
    "# print(avg_lines)\n",
    "# print(h_lines)\n",
    "merge_image = cv.addWeighted(img, 0.8, lines_image, 1,1)\n",
    "cv.imshow(\"result\",merge_image)\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()\n",
    "\n",
    "# cv.imshow(\"win\",lines_image)\n",
    "# cv.imshow(\"win\",canny_image)\n",
    "# cv.waitKey(0)\n",
    "# cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def average_slope_intercepts(img, lines):\n",
    "\n",
    "sometimes(in most cases actually) it detects multiple lines.  to overcome this, we can use the average value within the lines as you’ve seen in this code.\n",
    "\n",
    "bear in minds that, the lines (you can see at the ros function) is a lot of them. So you need to specify that only certain lines that is captured."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Turn Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main logic of this part is to make turn prediction using the central point between two lines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/@SunEdition/lane-detection-and-turn-prediction-algorithm-for-autonomous-vehicles-6423f77dc841"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/Infinem/Invenimus-Project/blob/master/lane_detection_with_memory/lane_detection_algorithm.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is this for?\n",
    "\n",
    "def display_heading_line(img, up_center, low_center):\n",
    "    # Draw a line from the top center to the bottom center\n",
    "    heading_image = np.zeros_like(img)\n",
    "    height, width, _ = img.shape\n",
    "    \n",
    "    x1 = int(low_center)\n",
    "    y1 = height\n",
    "    x2 = int(up_center)\n",
    "    y2 = int(height*0.72)\n",
    "    \n",
    "    cv.line(heading_image, (x1, y1), (x2, y2), (0, 0, 255), 5)\n",
    "    heading_image = cv.addWeighted(img, 0.8, heading_image, 1, 1)\n",
    "    \n",
    "    return heading_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_text(frame, image_center, left_x_base, right_x_base):\n",
    "    \"\"\" Function for text outputing\n",
    "    Output the direction of turn\"\"\"\n",
    "\n",
    "    lane_center = left_x_base + (right_x_base - left_x_base) / 2 # Find lane center between two lines\n",
    "    \n",
    "    deviation = image_center - lane_center    # Find the deviation\n",
    "\n",
    "    if deviation > 160:         # Prediction turn according to the deviation\n",
    "        text = \"Smooth Left\"\n",
    "        memory_text = text\n",
    "    elif deviation < 40 or deviation > 150 and deviation <= 160:\n",
    "        text = \"Smooth Right\"\n",
    "        memory_text = text\n",
    "    elif deviation >= 40 and deviation <= 150:\n",
    "        text = \"Straight\"\n",
    "        memory_text = text\n",
    "    else:\n",
    "        text = memory_text\n",
    "    \n",
    "    cv.putText(frame, \"DIRECTION: \" + text, (50, 50), cv.FONT_HERSHEY_DUPLEX, 1, (255, 0, 0), 2, cv.LINE_AA) # Draw direction\n",
    "    \n",
    "    return frame    # Retrun frame with the direction on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Declaring variables for fps\n",
    "#     avg_fps = 0\n",
    "#     fps_list = []\n",
    "    \n",
    "#     start_time = time.time()    # Start the timer\n",
    "\n",
    "#     edges = detect_edges(frame)\n",
    "\n",
    "#     denoised_frame = denoise_frame(frame)   # Denoise frame from artifacts\n",
    "\n",
    "#     canny_edges = detect_edges(denoised_frame)  # Find edges on the frame\n",
    "\n",
    "#     roi_frame = region_of_interest(canny_edges)   # Draw region of interest\n",
    "\n",
    "#     warped_frame = warp_perspective(canny_edges)    # Warp the original frame, make it skyview\n",
    "#     left_x_base, right_x_base = histogram(warped_frame)         # Take x bases for two lines\n",
    "#     lines = detect_lines(roi_frame)                 # Detect lane lines on the frame\n",
    "#     lane_lines = optimize_lines(frame, lines)       # Optimize detected line\n",
    "#     mul_lines = display_lines(frame, lines)\n",
    "#     lane_lines_image = display_lines(frame, lane_lines) # Display solid and optimized lines\n",
    "    \n",
    "#     up_center, low_center = get_floating_center(frame, lane_lines) # Calculate the center between two lines\n",
    "\n",
    "#     heading_line = display_heading_line(lane_lines_image, up_center, low_center)\n",
    "\n",
    "#     final_frame = add_text(heading_line, low_center, left_x_base, right_x_base) # Predict and draw turn\n",
    "\n",
    "#     fps = round(1.0 / (time.time() - start_time), 1)    # Here we calculate the fps\n",
    "#     fps_list.append(fps)           # Append fps to fps list\n",
    "#     cv2.putText(final_frame, \"FPS: \" + str(fps), (50, 100), cv2.FONT_HERSHEY_DUPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA) # Draw FPS\n",
    "\n",
    "#     if len(fps_list) == 60: # Calculate avg fps every timestamp\n",
    "#         avg_fps = round(sum(fps_list) / len(fps_list), 1)   # Averaging existing fps in the list\n",
    "#         fps_list = []\n",
    "#     cv2.putText(final_frame, \"AVG FPS: \" + str(avg_fps), (50, 150), cv2.FONT_HERSHEY_DUPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA) # Draw AVG FPS\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Direct Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pyautogui as py\n",
    "import pydirectinput\n",
    "\n",
    "# Function to click at specified coordinates (using PyAutoGUI)\n",
    "def click(x, y):\n",
    "    py.click(x, y)  # Click at the specified coordinates\n",
    "    time.sleep(0.5)  # Wait briefly to ensure the game is focused\n",
    "    \n",
    "def accelerate(hold_time):\n",
    "    start = time.time()\n",
    "    while time.time() - start < hold_time:\n",
    "        pydirectinput.keyDown('up')\n",
    "    pydirectinput.keyUp('up')\n",
    "\n",
    "def turnLeft(hold_time):\n",
    "    start = time.time()\n",
    "    while time.time() - start < hold_time:\n",
    "        pydirectinput.keyDown('a')\n",
    "    pydirectinput.keyUp('a')\n",
    "\n",
    "def turnRight(hold_time):\n",
    "    start = time.time()\n",
    "    while time.time() - start < hold_time:\n",
    "        pydirectinput.keyDown('d')\n",
    "    pydirectinput.keyUp('d')\n",
    "\n",
    "# Accelerate for 10 seconds, then turn left and right\n",
    "click(409, 275)\n",
    "accelerate(10)\n",
    "turnLeft(0.3)\n",
    "turnRight(0.3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
