{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code includes:\n",
    "\n",
    "1. Custom dataset class for COCO format\n",
    "2. Model initialization with pretrained weights\n",
    "3. Training and validation loops\n",
    "4. Learning rate scheduling\n",
    "5. Model checkpointing\n",
    "\n",
    "Key features:\n",
    "\n",
    "1. Uses Faster R-CNN with ResNet50 backbone\n",
    "2. Automatically maps your categories to model labels\n",
    "3. Saves the best model based on validation loss\n",
    "4. Shows progress bars during training\n",
    "5. Includes learning rate scheduling for better convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Screenshots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pyautogui\n",
    "from pynput import keyboard\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Screenshot capture started!\n",
      "Exiting program.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import keyboard\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Define a variable to control the screenshot loop\n",
    "capture = False\n",
    "\n",
    "while True:\n",
    "    # Check if the 's' key is pressed to start capturing\n",
    "    if keyboard.is_pressed('s'):\n",
    "        capture = True\n",
    "        print(\"Screenshot capture started!\")\n",
    "        time.sleep(0.2)  # Small delay to avoid multiple starts with one press\n",
    "\n",
    "    # Check if the 'q' key is pressed to stop capturing\n",
    "    if keyboard.is_pressed('q'):\n",
    "        capture = False\n",
    "        print(\"Screenshot capture stopped!\")\n",
    "        time.sleep(0.2)  # Small delay to avoid multiple stops with one press\n",
    "\n",
    "    # Take screenshots continuously if capture is True\n",
    "    if capture:\n",
    "        # Take a screenshot from the screen\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        screenshot = pyautogui.screenshot()\n",
    "        \n",
    "        # Convert screenshot to OpenCV format\n",
    "        screenshot_cv = cv2.cvtColor(np.array(screenshot), cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        # Save the screenshot with a unique timestamp\n",
    "        screenshot_path = f\"D:/Thinkin in programming/Metopen/Traffic Signs/{timestamp}.png\"\n",
    "        cv2.imwrite(screenshot_path, screenshot_cv)\n",
    "        \n",
    "        # Add a delay to control the frequency of screenshots\n",
    "        time.sleep(1)  # Take a screenshot every 1 second (adjust as needed)\n",
    "    \n",
    "    # Exit the loop if the ESC key is pressed\n",
    "    if keyboard.is_pressed('esc'):\n",
    "        print(\"Exiting program.\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/harshatejas/pytorch_custom_object_detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/trzy/FasterRCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2 as cv\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from pycocotools.coco import COCO\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# torch\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'TrafficSignDataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43;01mTrafficSignDataset\u001b[39;49;00m\u001b[43m(\u001b[49m\u001b[43mDataset\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroot_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mannotation_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransforms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mroot_dir\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 193\u001b[0m, in \u001b[0;36mTrafficSignDataset\u001b[1;34m()\u001b[0m\n\u001b[0;32m    189\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaved best model checpoint\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 193\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 129\u001b[0m, in \u001b[0;36mTrafficSignDataset.main\u001b[1;34m()\u001b[0m\n\u001b[0;32m    126\u001b[0m val_annotation \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD:/Thinkin in programming/Metopen/valid/_annotations.coco.json\u001b[39m\u001b[38;5;124m\"\u001b[39m       \n\u001b[0;32m    128\u001b[0m \u001b[38;5;66;03m# Create datasets\u001b[39;00m\n\u001b[1;32m--> 129\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mTrafficSignDataset\u001b[49m(root_dir, train_annotation)        \n\u001b[0;32m    130\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m TrafficSignDataset(root_dir, val_annotation)       \n\u001b[0;32m    133\u001b[0m \u001b[38;5;66;03m# Create data Loaders\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'TrafficSignDataset' is not defined"
     ]
    }
   ],
   "source": [
    "class TrafficSignDataset(Dataset):\n",
    "    def __init__(self, root_dir, annotation_file, transforms=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transforms = transforms\n",
    "\n",
    "        # Load COCO annotation\n",
    "        self.coco = COCO(annotation_file)\n",
    "\n",
    "        # get images ids\n",
    "        self.image_ids = list(sorted(self.coco.imgs.keys()))\n",
    "\n",
    "        # Get category mapping\n",
    "        self.category_ids = sorted(self.coco.getCatIds())\n",
    "        self.category_id_to_label = {cat_id: idx+1 for idx, cat_id in enumerate(self.category_ids)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load image\n",
    "        image_id = self.image_ids[idx]\n",
    "        image_info = self.coco.loadImgs(image_id)[0]\n",
    "        image_path = os.path.join(self.root_dir, image_info['file_name'])\n",
    "        image = cv.imread(image_path)\n",
    "        image = cv.cvtColor(image,cv.COLOR_BGR2RGB)\n",
    "\n",
    "        # Load annotations\n",
    "        ann_ids = self.coco.getAnnIds(imgIds=image_id)\n",
    "        anns = self.coco.loadAnns(ann_ids)\n",
    "\n",
    "        boxes = []\n",
    "        labels = []\n",
    "\n",
    "        for ann in anns:\n",
    "            # Coco format is [x,y, width, height]\n",
    "            # convert t [x1,y1,x2,y2]\n",
    "\n",
    "            x, y, w, h = ann['bbox']\n",
    "            x1 = x\n",
    "            y1 = y\n",
    "            x2 = x + w\n",
    "            y2 = y + h\n",
    "            boxes.append([x1,y1,x2,y2])\n",
    "\n",
    "\n",
    "            # Mqo category_id to our continues label index\n",
    "            label = self.category_id_to_label[ann['category_id']]\n",
    "            labels.append(label)\n",
    "\n",
    "        # Convert to tensor\n",
    "        boxes = torch.as_tensor(boxes, dtype= torch.float32)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "\n",
    "        target = {\n",
    "            'boxes': boxes,\n",
    "            'labels': labels,\n",
    "            'image_id' : torch.tensor([idx]),\n",
    "            'area': (boxes[:,3] - boxes[:,1]) * (boxes[:,2] - boxes[:,0]),\n",
    "            'iscrowd': torch.zeros((len(boxes), ), dtype=torch.int64)\n",
    "        }\n",
    "\n",
    "        image = torch.as_tensor(image, dtype=torch.float32) / 255.0\n",
    "        image = image.permute(2,0,1)\n",
    "\n",
    "        return image, target\n",
    "    \n",
    "    def get_model(num_classes):\n",
    "        # Load pre-trained model\n",
    "        model = fasterrcnn_resnet50_fpn(pretrained = True)\n",
    "\n",
    "        # get number f input features for the classifier\n",
    "        in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "\n",
    "        # Relace the pre-trained head with a new one\n",
    "        model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "        return model\n",
    "\n",
    "    def train_one_epoch(model, optimizer, data_loader, device):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for images, targets in tqdm(data_loader, desc=\"Training\"):\n",
    "            # move data to device\n",
    "            images = [images.to(device) for images in images]\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            # Forward pass\n",
    "            loss_dict = model(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            losses.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += losses.item()\n",
    "\n",
    "        return total_loss / len(data_loader)\n",
    "    \n",
    "    def validate(model, data_loader, device):\n",
    "        model.eval()\n",
    "        total_loss = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, targets in tqdm(data_loader, desc=\"Validation\"):\n",
    "                images  = [image.to(device) for image in images]\n",
    "                targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "\n",
    "                loss_dict = model(images, targets)\n",
    "                losses = sum(loss for loss in loss_dict.values())\n",
    "                total_loss += losses.item()\n",
    "\n",
    "        return total_loss / len(data_loader)\n",
    "    \n",
    "\n",
    "    def main():\n",
    "        #set device\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"Using device: {device}\")\n",
    "\n",
    "        # Dataset paths\n",
    "        root_dir= \"D:/Thinkin in programming/Metopen/Traffic Signs\"\n",
    "        train_annotation = \"D:/Thinkin in programming/Metopen/train/_annotations.coco.json\"\n",
    "        val_annotation = \"D:/Thinkin in programming/Metopen/valid/_annotations.coco.json\"       \n",
    "\n",
    "        # Create datasets\n",
    "        train_dataset = TrafficSignDataset(root_dir, train_annotation)        \n",
    "        val_dataset = TrafficSignDataset(root_dir, val_annotation)       \n",
    "\n",
    "\n",
    "        # Create data Loaders\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size =2,\n",
    "            shuffle=True,\n",
    "            collate_fn = lambda x: tuple(zip(*x)),\n",
    "            num_workers=4\n",
    "        )     \n",
    "\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size = 2,\n",
    "            shuffle = False,\n",
    "            collate_fn=lambda x: tuple(zip(*x)),\n",
    "            num_workers = 4\n",
    "        )\n",
    "\n",
    "        # Initialize model\n",
    "        num_classes = 4 # background + 3 sign types\n",
    "        model = get_model(num_classes)\n",
    "        model.to(device)\n",
    "\n",
    "        # Initialize optimizer\n",
    "        params = [p for p in model.parameters() if p.requires_grad]\n",
    "        optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "        # Initialize learning rate scheduler\n",
    "        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "\n",
    "        # Training loop\n",
    "        num_epoch = 10\n",
    "        best_loss = float('inf')\n",
    "\n",
    "        for epoch in range(num_epoch):\n",
    "            print(f\"nEpoch {epoch+1}/{num_epoch}\")\n",
    "\n",
    "            # Train\n",
    "            train_loss = train_one_epoch(model, optimizer, train_loader, device)\n",
    "            print(f\"Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "            # Validate\n",
    "            val_loss = validate(model, optimizer, val_loader, device)\n",
    "            print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "            # Update learning rate\n",
    "            lr_scheduler.step()\n",
    "\n",
    "            # Save best model\n",
    "            if val_loss < best_loss:\n",
    "                best_loss = val_loss\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'best_loss': best_loss,\n",
    "                }, 'best_model.pth')\n",
    "                print(\"Saved best model checpoint\")\n",
    "\n",
    "\n",
    "    if __name__ == \"__main__\":\n",
    "        main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "download CUDA here\n",
    "https://developer.nvidia.com/cuda-downloads?target_os=Windows&target_arch=x86_64&target_version=11&target_type=exe_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
